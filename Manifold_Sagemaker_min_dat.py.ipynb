{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "42b5e80b-ad1d-4335-a1f7-10a91127e3dc"
    }
   },
   "source": [
    "# Time series forecasting with DeepAR\n",
    "DeepAR is a supervised learning algorithm for forecasting scalar time series. \n",
    "\n",
    "\n",
    "Let's start by specifying:\n",
    "\n",
    "- The S3 bucket and prefix that you want to use for training and model data.  This should be within the same region as the Notebook Instance, training, and hosting.\n",
    "- The IAM role arn used to give training and hosting access to your data. See the documentation for how to create these.  Note, if more than one role is required for notebook instances, training, and/or hosting, please replace the boto regexp with a the appropriate full IAM role arn string(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "isConfigCell": true
   },
   "outputs": [],
   "source": [
    "bucket = 'sagemaker-testtimeseries'\n",
    "prefix = 'sagemaker/test-moredat'\n",
    " \n",
    "# Define IAM role\n",
    "import boto3\n",
    "import re\n",
    "from sagemaker import get_execution_role\n",
    "import json\n",
    "import math\n",
    "from os import path\n",
    "import pandas as pd\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "b2548d66-6f8f-426f-9cda-7a3cd1459abd"
    }
   },
   "source": [
    "Now we'll import the Python libraries we'll need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "nbpresent": {
     "id": "bb88eea9-27f3-4e47-9133-663911ea09a9"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import glob\n",
    "import sagemaker.amazon.common as smac\n",
    "import sagemaker\n",
    "from sagemaker.predictor import csv_serializer, json_deserializer\n",
    "import boto3\n",
    "import  s3fs\n",
    "import  sagemaker\n",
    "from sagemaker import get_execution_role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "\n",
    "s3_data_path = \"{}/{}/data\".format(bucket, prefix)\n",
    "s3_output_path = \"{}/{}/output\".format(bucket, prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "containers  = {\n",
    "    'us-west-2': '156387875391.dkr.ecr.us-west-2.amazonaws.com/forecasting-deepar:latest'\n",
    "}\n",
    "image_name = containers[boto3.Session().region_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arn:aws:iam::945416147148:role/service-role/AmazonSageMaker-ExecutionRole-20180616T095406'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "role"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "142777ae-c072-448e-b941-72bc75735d01"
    }
   },
   "source": [
    "---\n",
    "## Data\n",
    "\n",
    "Let's download the data.  More information about this dataset can be found [here](https://rdrr.io/github/robjhyndman/fpp/man/gasoline.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "nbpresent": {
     "id": "78105bc7-ce5d-4003-84f6-4dc5700c5945"
    }
   },
   "outputs": [],
   "source": [
    "#!wget http://gwa.ewi.tudelft.nl/fileadmin/pds/trace-archives/grid-workloads-archive/datasets/gwa-t-12/rnd.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import zipfile\n",
    "#with zipfile.ZipFile(\"rnd.zip\",\"r\") as zip_ref:\n",
    "#    zip_ref.extractall(\"targetdir\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "b472326f-3584-4b61-aecc-04b35486a1ab"
    }
   },
   "source": [
    "And take a look at it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USING FOR MORE DATA - 6/20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:3: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  app.launch_new_instance()\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:8: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:13: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n"
     ]
    }
   ],
   "source": [
    "files = glob.glob(os.path.join('targetdir/rnd/2013-7', \"1*.csv\"))\n",
    "#files_first200 = files[:150]\n",
    "dfs = [pd.read_csv(fp, sep = ';\\t').assign(VM=os.path.basename(fp).split('.')[0]) for fp in files]\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "files2 = glob.glob(os.path.join('targetdir/rnd/2013-8', \"1*.csv\"))\n",
    "#files2_first200 = files2[:150]\n",
    "dfs2 = [pd.read_csv(fp, sep = ';\\t').assign(VM=os.path.basename(fp).split('.')[0]) for fp in files2]\n",
    "df2 = pd.concat(dfs2, ignore_index=True)\n",
    "\n",
    "files3 = glob.glob(os.path.join('targetdir/rnd/2013-9', \"1*.csv\"))\n",
    "#files3_first200 = files3[:150]\n",
    "dfs3 = [pd.read_csv(fp, sep = ';\\t').assign(VM=os.path.basename(fp).split('.')[0]) for fp in files3]\n",
    "df3 = pd.concat(dfs3, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'files4 = glob.glob(os.path.join(\\'targetdir/rnd/2013-7\\', \"2*.csv\"))\\ndfs4 = [pd.read_csv(fp, sep = \\';\\t\\').assign(VM=os.path.basename(fp).split(\\'.\\')[0]) for fp in files4]\\ndf4 = pd.concat(dfs4, ignore_index=True)'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''files4 = glob.glob(os.path.join('targetdir/rnd/2013-7', \"2*.csv\"))\n",
    "dfs4 = [pd.read_csv(fp, sep = ';\\t').assign(VM=os.path.basename(fp).split('.')[0]) for fp in files4]\n",
    "df4 = pd.concat(dfs4, ignore_index=True)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'files5 = glob.glob(os.path.join(\\'targetdir/rnd/2013-8\\', \"2*.csv\"))\\ndfs5 = [pd.read_csv(fp, sep = \\';\\t\\').assign(VM=os.path.basename(fp).split(\\'.\\')[0]) for fp in files5]\\ndf5 = pd.concat(dfs5, ignore_index=True)'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''files5 = glob.glob(os.path.join('targetdir/rnd/2013-8', \"2*.csv\"))\n",
    "dfs5 = [pd.read_csv(fp, sep = ';\\t').assign(VM=os.path.basename(fp).split('.')[0]) for fp in files5]\n",
    "df5 = pd.concat(dfs5, ignore_index=True)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'files6 = glob.glob(os.path.join(\\'targetdir/rnd/2013-9\\', \"2*.csv\"))\\ndfs6 = [pd.read_csv(fp, sep = \\';\\t\\').assign(VM=os.path.basename(fp).split(\\'.\\')[0]) for fp in files6]\\ndf6 = pd.concat(dfs6, ignore_index=True)'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''files6 = glob.glob(os.path.join('targetdir/rnd/2013-9', \"2*.csv\"))\n",
    "dfs6 = [pd.read_csv(fp, sep = ';\\t').assign(VM=os.path.basename(fp).split('.')[0]) for fp in files6]\n",
    "df6 = pd.concat(dfs6, ignore_index=True)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nnewdat2 = df.append(df4)\\nnewerdat2 = newdat2.append(df5)\\nconcatenated_df2 = newerdat2\\n\\nconcatenated_df = concatenated_df1.append(concatenated_df2)'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newdat = df.append(df2)\n",
    "newerdat = newdat.append(df3)\n",
    "concatenated_df = newerdat\n",
    "'''\n",
    "newdat2 = df.append(df4)\n",
    "newerdat2 = newdat2.append(df5)\n",
    "concatenated_df2 = newerdat2\n",
    "\n",
    "concatenated_df = concatenated_df1.append(concatenated_df2)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp [ms]</th>\n",
       "      <th>CPU cores</th>\n",
       "      <th>CPU capacity provisioned [MHZ]</th>\n",
       "      <th>CPU usage [MHZ]</th>\n",
       "      <th>CPU usage [%]</th>\n",
       "      <th>Memory capacity provisioned [KB]</th>\n",
       "      <th>Memory usage [KB]</th>\n",
       "      <th>Disk read throughput [KB/s]</th>\n",
       "      <th>Disk write throughput [KB/s]</th>\n",
       "      <th>Network received throughput [KB/s]</th>\n",
       "      <th>Network transmitted throughput [KB/s]</th>\n",
       "      <th>VM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1372629804</td>\n",
       "      <td>1</td>\n",
       "      <td>2599.999398</td>\n",
       "      <td>41.599990</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>2097152.0</td>\n",
       "      <td>260044.800000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>7.8</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>7.400000</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1372630104</td>\n",
       "      <td>1</td>\n",
       "      <td>2599.999398</td>\n",
       "      <td>36.399992</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>2097152.0</td>\n",
       "      <td>281016.800000</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1372630404</td>\n",
       "      <td>1</td>\n",
       "      <td>2599.999398</td>\n",
       "      <td>25.999994</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2097152.0</td>\n",
       "      <td>209712.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>7.4</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.466667</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1372630704</td>\n",
       "      <td>1</td>\n",
       "      <td>2599.999398</td>\n",
       "      <td>31.199993</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>2097152.0</td>\n",
       "      <td>209712.000000</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>6.8</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1372631004</td>\n",
       "      <td>1</td>\n",
       "      <td>2599.999398</td>\n",
       "      <td>29.466660</td>\n",
       "      <td>1.133333</td>\n",
       "      <td>2097152.0</td>\n",
       "      <td>254453.066667</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.466667</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Timestamp [ms]  CPU cores  CPU capacity provisioned [MHZ]  CPU usage [MHZ]  \\\n",
       "0      1372629804          1                     2599.999398        41.599990   \n",
       "1      1372630104          1                     2599.999398        36.399992   \n",
       "2      1372630404          1                     2599.999398        25.999994   \n",
       "3      1372630704          1                     2599.999398        31.199993   \n",
       "4      1372631004          1                     2599.999398        29.466660   \n",
       "\n",
       "   CPU usage [%]  Memory capacity provisioned [KB]  Memory usage [KB]  \\\n",
       "0       1.600000                         2097152.0      260044.800000   \n",
       "1       1.400000                         2097152.0      281016.800000   \n",
       "2       1.000000                         2097152.0      209712.000000   \n",
       "3       1.200000                         2097152.0      209712.000000   \n",
       "4       1.133333                         2097152.0      254453.066667   \n",
       "\n",
       "   Disk read throughput [KB/s]  Disk write throughput [KB/s]  \\\n",
       "0                     0.400000                           7.8   \n",
       "1                     0.133333                          10.0   \n",
       "2                     0.200000                           7.4   \n",
       "3                     0.066667                           6.8   \n",
       "4                     0.133333                           7.0   \n",
       "\n",
       "   Network received throughput [KB/s]  Network transmitted throughput [KB/s]  \\\n",
       "0                            1.666667                               7.400000   \n",
       "1                            1.000000                               2.000000   \n",
       "2                            1.000000                               1.466667   \n",
       "3                            1.000000                               1.400000   \n",
       "4                            1.000000                               1.466667   \n",
       "\n",
       "   VM  \n",
       "0  13  \n",
       "1  13  \n",
       "2  13  \n",
       "3  13  \n",
       "4  13  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concatenated_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp [ms]</th>\n",
       "      <th>CPU cores</th>\n",
       "      <th>CPU capacity provisioned [MHZ]</th>\n",
       "      <th>CPU usage [MHZ]</th>\n",
       "      <th>CPU usage [%]</th>\n",
       "      <th>Memory capacity provisioned [KB]</th>\n",
       "      <th>Memory usage [KB]</th>\n",
       "      <th>Disk read throughput [KB/s]</th>\n",
       "      <th>Disk write throughput [KB/s]</th>\n",
       "      <th>Network received throughput [KB/s]</th>\n",
       "      <th>Network transmitted throughput [KB/s]</th>\n",
       "      <th>VM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>926201</th>\n",
       "      <td>1380491363</td>\n",
       "      <td>4</td>\n",
       "      <td>11703.997296</td>\n",
       "      <td>150.201299</td>\n",
       "      <td>1.283333</td>\n",
       "      <td>16703488.0</td>\n",
       "      <td>167772.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>926202</th>\n",
       "      <td>1380491630</td>\n",
       "      <td>4</td>\n",
       "      <td>11703.997296</td>\n",
       "      <td>197.017288</td>\n",
       "      <td>1.683333</td>\n",
       "      <td>16703488.0</td>\n",
       "      <td>100663.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>926203</th>\n",
       "      <td>1380491663</td>\n",
       "      <td>4</td>\n",
       "      <td>11703.997296</td>\n",
       "      <td>197.017288</td>\n",
       "      <td>1.683333</td>\n",
       "      <td>16703488.0</td>\n",
       "      <td>100663.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>926204</th>\n",
       "      <td>1380491930</td>\n",
       "      <td>4</td>\n",
       "      <td>11703.997296</td>\n",
       "      <td>146.299966</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>16703488.0</td>\n",
       "      <td>111848.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>926205</th>\n",
       "      <td>1380491963</td>\n",
       "      <td>4</td>\n",
       "      <td>11703.997296</td>\n",
       "      <td>146.299966</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>16703488.0</td>\n",
       "      <td>111848.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Timestamp [ms]  CPU cores  CPU capacity provisioned [MHZ]  \\\n",
       "926201      1380491363          4                    11703.997296   \n",
       "926202      1380491630          4                    11703.997296   \n",
       "926203      1380491663          4                    11703.997296   \n",
       "926204      1380491930          4                    11703.997296   \n",
       "926205      1380491963          4                    11703.997296   \n",
       "\n",
       "        CPU usage [MHZ]  CPU usage [%]  Memory capacity provisioned [KB]  \\\n",
       "926201       150.201299       1.283333                        16703488.0   \n",
       "926202       197.017288       1.683333                        16703488.0   \n",
       "926203       197.017288       1.683333                        16703488.0   \n",
       "926204       146.299966       1.250000                        16703488.0   \n",
       "926205       146.299966       1.250000                        16703488.0   \n",
       "\n",
       "        Memory usage [KB]  Disk read throughput [KB/s]  \\\n",
       "926201           167772.0                          0.0   \n",
       "926202           100663.2                          0.0   \n",
       "926203           100663.2                          0.0   \n",
       "926204           111848.0                          0.0   \n",
       "926205           111848.0                          0.0   \n",
       "\n",
       "        Disk write throughput [KB/s]  Network received throughput [KB/s]  \\\n",
       "926201                      0.066667                                 0.0   \n",
       "926202                      1.666667                                 0.0   \n",
       "926203                      1.666667                                 0.0   \n",
       "926204                      0.000000                                 0.0   \n",
       "926205                      0.000000                                 0.0   \n",
       "\n",
       "        Network transmitted throughput [KB/s]   VM  \n",
       "926201                                    0.0  106  \n",
       "926202                                    0.0  106  \n",
       "926203                                    0.0  106  \n",
       "926204                                    0.0  106  \n",
       "926205                                    0.0  106  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concatenated_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_df['Timestamp'] = pd.to_datetime(concatenated_df['Timestamp [ms]'], unit = 's')\n",
    "#concatenated_df.apply(pd.to_numeric, errors='ignore')\n",
    "concatenated_df.describe()\n",
    "concatenated_df['weekday'] = concatenated_df['Timestamp'].dt.dayofweek\n",
    "\n",
    "\n",
    "concatenated_df['weekend'] = ((concatenated_df.weekday) // 5 == 1).astype(float)\n",
    "# Feature engineering with the date\n",
    "concatenated_df['month']=concatenated_df.Timestamp.dt.month \n",
    "concatenated_df['day']=concatenated_df.Timestamp.dt.day\n",
    "concatenated_df.set_index('Timestamp',inplace=True)\n",
    "concatenated_df[\"CPU usage prev\"] = concatenated_df['CPU usage [%]'].shift(1)\n",
    "concatenated_df[\"CPU_diff\"] = concatenated_df['CPU usage [%]'] - concatenated_df[\"CPU usage prev\"]\n",
    "concatenated_df[\"received_prev\"] = concatenated_df['Network received throughput [KB/s]'].shift(1)\n",
    "concatenated_df[\"received_diff\"] = concatenated_df['Network received throughput [KB/s]']- concatenated_df[\"received_prev\"]\n",
    "\n",
    "concatenated_df[\"transmitted_prev\"] = concatenated_df['Network transmitted throughput [KB/s]'].shift(1)\n",
    "concatenated_df[\"transmitted_diff\"] = concatenated_df['Network transmitted throughput [KB/s]']- concatenated_df[\"transmitted_prev\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "concatenated_df[\"start\"] = concatenated_df.index\n",
    "concatenated_df['target'] = concatenated_df['CPU usage [MHZ]']\n",
    "#jsondat=pd.DataFrame(concatenated_df['start'])\n",
    "#jsondat['start'] = concatenated_df.index\n",
    "#result = pd.concat([concatenated_df['start'], concatenated_df['target']], axis=1)\n",
    "#result = result.reset_index()\n",
    "#result.drop('Timestamp', axis=1, inplace=True)\n",
    "#result.start = result.start.astype(str)\n",
    "#result.set_index('start',inplace=True)\n",
    "#result\n",
    "\n",
    "#result.set_index('start',inplace=True)\n",
    "#result.index = pd.to_datetime(result.index) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "result = pd.concat([concatenated_df['VM'],concatenated_df['start'], concatenated_df['target']], axis=1)\n",
    "result = result.reset_index()\n",
    "#result.set_index('start',inplace=True)\n",
    "\n",
    "result.set_index(['start', 'VM'],inplace=True)\n",
    "result.drop('Timestamp', axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/core/groupby/groupby.py:1502: FutureWarning: how in .resample() is deprecated\n",
      "the new syntax is .resample(...)..apply(<func>)\n",
      "  return get_resampler_for_grouping(self, rule, *args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "df = concatenated_df.groupby('VM').resample('1min', how={'target':np.mean})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(level=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['Timestamp'] = df['Timestamp'].dt.strftime('%m/%d/%Y %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = \"1min\"\n",
    "context_length = 50\n",
    "prediction_length = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftrain = df[:-(prediction_length)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VM</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2013-09-29 21:04:00</th>\n",
       "      <td>199</td>\n",
       "      <td>86.666653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-09-29 21:05:00</th>\n",
       "      <td>199</td>\n",
       "      <td>86.666653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-09-29 21:06:00</th>\n",
       "      <td>199</td>\n",
       "      <td>86.666653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-09-29 21:07:00</th>\n",
       "      <td>199</td>\n",
       "      <td>86.666653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-09-29 21:08:00</th>\n",
       "      <td>199</td>\n",
       "      <td>86.666653</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      VM     target\n",
       "Timestamp                          \n",
       "2013-09-29 21:04:00  199  86.666653\n",
       "2013-09-29 21:05:00  199  86.666653\n",
       "2013-09-29 21:06:00  199  86.666653\n",
       "2013-09-29 21:07:00  199  86.666653\n",
       "2013-09-29 21:08:00  199  86.666653"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dftrain.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VM</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2013-09-29 21:54:00</th>\n",
       "      <td>199</td>\n",
       "      <td>84.933320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-09-29 21:55:00</th>\n",
       "      <td>199</td>\n",
       "      <td>84.933320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-09-29 21:56:00</th>\n",
       "      <td>199</td>\n",
       "      <td>84.933320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-09-29 21:57:00</th>\n",
       "      <td>199</td>\n",
       "      <td>84.933320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-09-29 21:58:00</th>\n",
       "      <td>199</td>\n",
       "      <td>102.266651</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      VM      target\n",
       "Timestamp                           \n",
       "2013-09-29 21:54:00  199   84.933320\n",
       "2013-09-29 21:55:00  199   84.933320\n",
       "2013-09-29 21:56:00  199   84.933320\n",
       "2013-09-29 21:57:00  199   84.933320\n",
       "2013-09-29 21:58:00  199  102.266651"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def series_to_obj(ts, cat=None):\n",
    "    obj = {\"start\": str(ts.index[0]), \"target\": list(ts)}\n",
    "    if cat is not None:\n",
    "        obj[\"cat\"] = cat\n",
    "    return obj\n",
    "\n",
    "def series_to_jsonline(ts, cat=None):\n",
    "    return json.dumps(series_to_obj(ts, cat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nwith open('./test_data.json', 'wb') as fp:\\n    for ts in time_series_test:\\n        fp.write(series_to_jsonline(ts).encode(encoding))\\n        fp.write('\\n'.encode(encoding)) \""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_series_test=[]\n",
    "vm_index_range = df['VM'].unique()\n",
    "for i in vm_index_range:\n",
    "    newseries = df[df['VM'] == i]['target']\n",
    "    del newseries.index.name\n",
    "    newseries.index = pd.to_datetime(newseries.index)\n",
    "    time_series_test.append(newseries)\n",
    "    #time_series_test = time_series_test.fillna(0)\n",
    "    \n",
    "'''\n",
    "with open('./test_data.json', 'wb') as fp:\n",
    "    for ts in time_series_test:\n",
    "        fp.write(series_to_jsonline(ts).encode(encoding))\n",
    "        fp.write('\\n'.encode(encoding)) '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_training=[]\n",
    "vm_index_range = df['VM'].unique()\n",
    "for i in vm_index_range:\n",
    "    newseries = df[df['VM'] == i]['target']\n",
    "    del newseries.index.name\n",
    "    newseries.index = pd.to_datetime(newseries.index)\n",
    "    time_series_training.append(newseries[:-prediction_length])\n",
    "    #time_series_training = time_series_training.fillna(0)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"with open('./train_data.json', 'wb') as fp:\\n    for ts in time_series_training:\\n        fp.write(series_to_jsonline(ts).encode(encoding))\\n        fp.write('\\n'.encode(encoding)) \""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''with open('./train_data.json', 'wb') as fp:\n",
    "    for ts in time_series_training:\n",
    "        fp.write(series_to_jsonline(ts).encode(encoding))\n",
    "        fp.write('\\n'.encode(encoding)) '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## can't figure out how to push the json file to S3 bucket... manually moved for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3filesystem = s3fs.S3FileSystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<s3fs.core.S3FileSystem at 0x7f5b48c5d4e0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3filesystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = \"utf-8\"\n",
    "\n",
    "with s3filesystem.open(s3_data_path + \"/test/test_data.json\", 'wb') as fp:\n",
    "    for ts in time_series_test:\n",
    "        fp.write(series_to_jsonline(ts).encode(encoding))\n",
    "        fp.write('\\n'.encode(encoding)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with s3filesystem.open(s3_data_path + \"/train/train_data.json\", 'wb') as fp:\n",
    "    for ts in time_series_training:\n",
    "        fp.write(series_to_jsonline(ts).encode(encoding))\n",
    "        fp.write('\\n'.encode(encoding)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sagemaker-testtimeseries/sagemaker/test-moredat/data'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3_data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = sagemaker.estimator.Estimator(\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    image_name=image_name,\n",
    "    role=role,\n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.c4.xlarge',\n",
    "    base_job_name='test-demo-deepar',\n",
    "    output_path=\"s3://\" + s3_output_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sagemaker automatically adds lags, so context length can be shorter than seasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hyperparameters  = {\n",
    "    \"time_freq\": freq,\n",
    "    \"context_length\": context_length,\n",
    "    \"prediction_length\": prediction_length,\n",
    "    \"num_cells\": \"40\",\n",
    "    \"num_layers\": \"3\",\n",
    "    \"likelihood\": \"student-t\",\n",
    "    \"epochs\": \"20\",\n",
    "    \"mini_batch_size\": \"32\",\n",
    "    \"learning_rate\": \"0.001\",\n",
    "    \"dropout_rate\": \"0.05\",\n",
    "    \"early_stopping_patience\": \"10\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.set_hyperparameters(**hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sagemaker-testtimeseries/sagemaker/test-moredat/data'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3_data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: test-demo-deepar-2018-06-21-03-55-48-846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "......................\n",
      "\u001b[31mArguments: train\u001b[0m\n",
      "\u001b[31m[06/21/2018 03:59:20 INFO 140419976746816] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/algorithm/default-input.json: {u'dropout_rate': u'0.10', u'cardinality': u'', u'test_quantiles': u'[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', u'_tuning_objective_metric': u'', u'_num_gpus': u'auto', u'learning_rate': u'0.001', u'num_cells': u'40', u'num_layers': u'2', u'embedding_dimension': u'', u'_kvstore': u'auto', u'_num_kv_servers': u'auto', u'mini_batch_size': u'32', u'likelihood': u'student-t', u'early_stopping_patience': u''}\u001b[0m\n",
      "\u001b[31m[06/21/2018 03:59:20 INFO 140419976746816] Reading provided configuration from /opt/ml/input/config/hyperparameters.json: {u'dropout_rate': u'0.05', u'learning_rate': u'0.001', u'num_cells': u'40', u'prediction_length': u'50', u'epochs': u'20', u'time_freq': u'1min', u'context_length': u'50', u'num_layers': u'3', u'mini_batch_size': u'32', u'likelihood': u'student-t', u'early_stopping_patience': u'10'}\u001b[0m\n",
      "\u001b[31m[06/21/2018 03:59:20 INFO 140419976746816] Final configuration: {u'dropout_rate': u'0.05', u'test_quantiles': u'[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', u'_tuning_objective_metric': u'', u'learning_rate': u'0.001', u'num_layers': u'3', u'epochs': u'20', u'embedding_dimension': u'', u'num_cells': u'40', u'_num_kv_servers': u'auto', u'cardinality': u'', u'likelihood': u'student-t', u'mini_batch_size': u'32', u'_num_gpus': u'auto', u'prediction_length': u'50', u'time_freq': u'1min', u'context_length': u'50', u'_kvstore': u'auto', u'early_stopping_patience': u'10'}\u001b[0m\n",
      "\u001b[31m[06/21/2018 03:59:20 INFO 140419976746816] Detected entry point for worker worker\u001b[0m\n",
      "\u001b[31m[06/21/2018 03:59:20 INFO 140419976746816] nvidia-smi took: 0.0251660346985 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[31m[06/21/2018 03:59:20 INFO 140419976746816] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[31m[06/21/2018 03:59:21 INFO 140419976746816] Training set statistics:\u001b[0m\n",
      "\u001b[31m[06/21/2018 03:59:21 INFO 140419976746816] Real timeseries\u001b[0m\n",
      "\u001b[31m[06/21/2018 03:59:21 INFO 140419976746816] number of timeseries: 111\u001b[0m\n",
      "\u001b[31m[06/21/2018 03:59:21 INFO 140419976746816] number of observations: 13316715\u001b[0m\n",
      "\u001b[31m[06/21/2018 03:59:21 INFO 140419976746816] mean target length: 119970\u001b[0m\n",
      "\u001b[31m[06/21/2018 03:59:21 INFO 140419976746816] min/mean/max target: 0.0/376.707365761/11454.3125\u001b[0m\n",
      "\u001b[31m[06/21/2018 03:59:21 INFO 140419976746816] mean abs(target): 376.707365761\u001b[0m\n",
      "\u001b[31m[06/21/2018 03:59:21 INFO 140419976746816] Small number of time-series. Doing 3 number of passes over dataset per epoch.\u001b[0m\n",
      "\u001b[31m[06/21/2018 03:59:22 INFO 140419976746816] Test set statistics:\u001b[0m\n",
      "\u001b[31m[06/21/2018 03:59:22 INFO 140419976746816] Real timeseries\u001b[0m\n",
      "\u001b[31m[06/21/2018 03:59:22 INFO 140419976746816] number of timeseries: 111\u001b[0m\n",
      "\u001b[31m[06/21/2018 03:59:22 INFO 140419976746816] number of observations: 13322265\u001b[0m\n",
      "\u001b[31m[06/21/2018 03:59:22 INFO 140419976746816] mean target length: 120020\u001b[0m\n",
      "\u001b[31m[06/21/2018 03:59:22 INFO 140419976746816] min/mean/max target: 0.0/376.60888636/11454.3125\u001b[0m\n",
      "\u001b[31m[06/21/2018 03:59:22 INFO 140419976746816] mean abs(target): 376.60888636\u001b[0m\n",
      "\u001b[31m[06/21/2018 03:59:22 INFO 140419976746816] Create Store: local\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"get_graph.time\": {\"count\": 1, \"max\": 322.5231170654297, \"sum\": 322.5231170654297, \"min\": 322.5231170654297}}, \"EndTime\": 1529553563.115285, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1529553562.790645}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/21/2018 03:59:23 INFO 140419976746816] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"initialize.time\": {\"count\": 1, \"max\": 717.642068862915, \"sum\": 717.642068862915, \"min\": 717.642068862915}}, \"EndTime\": 1529553563.508353, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1529553563.11535}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/21/2018 03:59:23 INFO 140419976746816] Using early stopping with patience 10\u001b[0m\n",
      "\u001b[31m[06/21/2018 03:59:24 INFO 140419976746816] Epoch[0] Batch[0] avg_epoch_loss=7.422563\u001b[0m\n",
      "\u001b[31m[06/21/2018 03:59:26 INFO 140419976746816] Epoch[0] Batch[5] avg_epoch_loss=6.237973\u001b[0m\n",
      "\u001b[31m[06/21/2018 03:59:26 INFO 140419976746816] Epoch[0] Batch [5]#011Speed: 74.63 samples/sec#011loss=6.237973\u001b[0m\n",
      "\u001b[31m[06/21/2018 03:59:28 INFO 140419976746816] Epoch[0] Batch[10] avg_epoch_loss=5.933668\u001b[0m\n",
      "\u001b[31m[06/21/2018 03:59:28 INFO 140419976746816] Epoch[0] Batch [10]#011Speed: 69.16 samples/sec#011loss=5.568501\u001b[0m\n",
      "\u001b[31m[06/21/2018 03:59:28 INFO 140419976746816] processed a total of 335 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"epochs\": {\"count\": 1, \"max\": 20, \"sum\": 20.0, \"min\": 20}, \"update.time\": {\"count\": 1, \"max\": 5174.878120422363, \"sum\": 5174.878120422363, \"min\": 5174.878120422363}}, \"EndTime\": 1529553568.699887, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1529553563.508419}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/21/2018 03:59:28 INFO 140419976746816] #throughput_metric: host=algo-1, train throughput=64.7343985324 records/second\u001b[0m\n",
      "\u001b[31m[06/21/2018 03:59:28 INFO 140419976746816] #progress_metric: host=algo-1, completed 5 % of epochs\u001b[0m\n",
      "\u001b[31m[06/21/2018 03:59:28 INFO 140419976746816] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[06/21/2018 03:59:28 INFO 140419976746816] Saved checkpoint to \"/opt/ml/model/state_8cd25711-9110-4382-8776-ea19541d0775-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 39.26515579223633, \"sum\": 39.26515579223633, \"min\": 39.26515579223633}}, \"EndTime\": 1529553568.739618, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1529553568.699962}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/21/2018 03:59:29 INFO 140419976746816] Epoch[1] Batch[0] avg_epoch_loss=5.845622\u001b[0m\n",
      "\u001b[31m[06/21/2018 03:59:31 INFO 140419976746816] Epoch[1] Batch[5] avg_epoch_loss=5.456105\u001b[0m\n",
      "\u001b[31m[06/21/2018 03:59:31 INFO 140419976746816] Epoch[1] Batch [5]#011Speed: 71.38 samples/sec#011loss=5.456105\u001b[0m\n",
      "\u001b[31m[06/21/2018 03:59:33 INFO 140419976746816] Epoch[1] Batch[10] avg_epoch_loss=5.230202\u001b[0m\n",
      "\u001b[31m[06/21/2018 03:59:33 INFO 140419976746816] Epoch[1] Batch [10]#011Speed: 69.95 samples/sec#011loss=4.959119\u001b[0m\n",
      "\u001b[31m[06/21/2018 03:59:33 INFO 140419976746816] processed a total of 346 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4969.671010971069, \"sum\": 4969.671010971069, \"min\": 4969.671010971069}}, \"EndTime\": 1529553573.723314, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1529553568.739689}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/21/2018 03:59:33 INFO 140419976746816] #throughput_metric: host=algo-1, train throughput=69.6200036047 records/second\u001b[0m\n",
      "\u001b[31m[06/21/2018 03:59:33 INFO 140419976746816] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[31m[06/21/2018 03:59:33 INFO 140419976746816] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[06/21/2018 03:59:33 INFO 140419976746816] Saved checkpoint to \"/opt/ml/model/state_040b12fa-e179-4a0c-983a-070961bd7525-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 42.92106628417969, \"sum\": 42.92106628417969, \"min\": 42.92106628417969}}, \"EndTime\": 1529553573.766721, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1529553573.723444}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/21/2018 03:59:34 INFO 140419976746816] Epoch[2] Batch[0] avg_epoch_loss=4.670977\u001b[0m\n",
      "\u001b[31m[06/21/2018 03:59:36 INFO 140419976746816] Epoch[2] Batch[5] avg_epoch_loss=4.630873\u001b[0m\n",
      "\u001b[31m[06/21/2018 03:59:36 INFO 140419976746816] Epoch[2] Batch [5]#011Speed: 64.75 samples/sec#011loss=4.630873\u001b[0m\n",
      "\u001b[31m[06/21/2018 03:59:38 INFO 140419976746816] Epoch[2] Batch[10] avg_epoch_loss=4.616240\u001b[0m\n",
      "\u001b[31m[06/21/2018 03:59:38 INFO 140419976746816] Epoch[2] Batch [10]#011Speed: 77.40 samples/sec#011loss=4.598681\u001b[0m\n",
      "\u001b[31m[06/21/2018 03:59:38 INFO 140419976746816] processed a total of 335 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4985.34893989563, \"sum\": 4985.34893989563, \"min\": 4985.34893989563}}, \"EndTime\": 1529553578.765565, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1529553573.766785}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/21/2018 03:59:38 INFO 140419976746816] #throughput_metric: host=algo-1, train throughput=67.1954325778 records/second\u001b[0m\n",
      "\u001b[31m[06/21/2018 03:59:38 INFO 140419976746816] #progress_metric: host=algo-1, completed 15 % of epochs\u001b[0m\n",
      "\u001b[31m[06/21/2018 03:59:38 INFO 140419976746816] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[06/21/2018 03:59:38 INFO 140419976746816] Saved checkpoint to \"/opt/ml/model/state_a7e00abc-3275-480b-a6cb-c3d941688900-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 40.19618034362793, \"sum\": 40.19618034362793, \"min\": 40.19618034362793}}, \"EndTime\": 1529553578.806212, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1529553578.765637}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/21/2018 03:59:39 INFO 140419976746816] Epoch[3] Batch[0] avg_epoch_loss=4.338840\u001b[0m\n",
      "\u001b[31m[06/21/2018 03:59:41 INFO 140419976746816] Epoch[3] Batch[5] avg_epoch_loss=4.473562\u001b[0m\n",
      "\u001b[31m[06/21/2018 03:59:41 INFO 140419976746816] Epoch[3] Batch [5]#011Speed: 66.12 samples/sec#011loss=4.473562\u001b[0m\n",
      "\u001b[31m[06/21/2018 03:59:43 INFO 140419976746816] Epoch[3] Batch[10] avg_epoch_loss=4.426733\u001b[0m\n",
      "\u001b[31m[06/21/2018 03:59:43 INFO 140419976746816] Epoch[3] Batch [10]#011Speed: 80.57 samples/sec#011loss=4.370538\u001b[0m\n",
      "\u001b[31m[06/21/2018 03:59:43 INFO 140419976746816] processed a total of 324 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4975.575923919678, \"sum\": 4975.575923919678, \"min\": 4975.575923919678}}, \"EndTime\": 1529553583.795733, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1529553578.806263}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/21/2018 03:59:43 INFO 140419976746816] #throughput_metric: host=algo-1, train throughput=65.1156155206 records/second\u001b[0m\n",
      "\u001b[31m[06/21/2018 03:59:43 INFO 140419976746816] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[31m[06/21/2018 03:59:43 INFO 140419976746816] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[06/21/2018 03:59:43 INFO 140419976746816] Saved checkpoint to \"/opt/ml/model/state_59d8b55a-db8c-4b0b-9422-67944b617120-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 39.29591178894043, \"sum\": 39.29591178894043, \"min\": 39.29591178894043}}, \"EndTime\": 1529553583.835485, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1529553583.795803}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/21/2018 03:59:44 INFO 140419976746816] Epoch[4] Batch[0] avg_epoch_loss=4.342124\u001b[0m\n",
      "\u001b[31m[06/21/2018 03:59:46 INFO 140419976746816] Epoch[4] Batch[5] avg_epoch_loss=4.300388\u001b[0m\n",
      "\u001b[31m[06/21/2018 03:59:46 INFO 140419976746816] Epoch[4] Batch [5]#011Speed: 75.82 samples/sec#011loss=4.300388\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[06/21/2018 03:59:48 INFO 140419976746816] Epoch[4] Batch[10] avg_epoch_loss=4.349163\u001b[0m\n",
      "\u001b[31m[06/21/2018 03:59:48 INFO 140419976746816] Epoch[4] Batch [10]#011Speed: 75.77 samples/sec#011loss=4.407693\u001b[0m\n",
      "\u001b[31m[06/21/2018 03:59:48 INFO 140419976746816] processed a total of 368 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4974.170923233032, \"sum\": 4974.170923233032, \"min\": 4974.170923233032}}, \"EndTime\": 1529553588.822998, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1529553583.835558}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/21/2018 03:59:48 INFO 140419976746816] #throughput_metric: host=algo-1, train throughput=73.9807031394 records/second\u001b[0m\n",
      "\u001b[31m[06/21/2018 03:59:48 INFO 140419976746816] #progress_metric: host=algo-1, completed 25 % of epochs\u001b[0m\n",
      "\u001b[31m[06/21/2018 03:59:48 INFO 140419976746816] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[06/21/2018 03:59:48 INFO 140419976746816] Saved checkpoint to \"/opt/ml/model/state_de74cd15-c888-4796-98a7-ca4f34bded16-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 40.25101661682129, \"sum\": 40.25101661682129, \"min\": 40.25101661682129}}, \"EndTime\": 1529553588.863642, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1529553588.823062}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/21/2018 03:59:49 INFO 140419976746816] Epoch[5] Batch[0] avg_epoch_loss=4.406187\u001b[0m\n",
      "\u001b[31m[06/21/2018 03:59:51 INFO 140419976746816] Epoch[5] Batch[5] avg_epoch_loss=4.294793\u001b[0m\n",
      "\u001b[31m[06/21/2018 03:59:51 INFO 140419976746816] Epoch[5] Batch [5]#011Speed: 62.11 samples/sec#011loss=4.294793\u001b[0m\n",
      "\u001b[31m[06/21/2018 03:59:53 INFO 140419976746816] processed a total of 304 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4833.544969558716, \"sum\": 4833.544969558716, \"min\": 4833.544969558716}}, \"EndTime\": 1529553593.711051, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1529553588.863705}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/21/2018 03:59:53 INFO 140419976746816] #throughput_metric: host=algo-1, train throughput=62.8923552747 records/second\u001b[0m\n",
      "\u001b[31m[06/21/2018 03:59:53 INFO 140419976746816] #progress_metric: host=algo-1, completed 30 % of epochs\u001b[0m\n",
      "\u001b[31m[06/21/2018 03:59:53 INFO 140419976746816] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[06/21/2018 03:59:53 INFO 140419976746816] Saved checkpoint to \"/opt/ml/model/state_a3e9f941-dd43-43cd-af1d-185c3424bea0-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 38.47098350524902, \"sum\": 38.47098350524902, \"min\": 38.47098350524902}}, \"EndTime\": 1529553593.749954, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1529553593.711124}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/21/2018 03:59:54 INFO 140419976746816] Epoch[6] Batch[0] avg_epoch_loss=4.044627\u001b[0m\n",
      "\u001b[31m[06/21/2018 03:59:56 INFO 140419976746816] Epoch[6] Batch[5] avg_epoch_loss=4.137589\u001b[0m\n",
      "\u001b[31m[06/21/2018 03:59:56 INFO 140419976746816] Epoch[6] Batch [5]#011Speed: 65.29 samples/sec#011loss=4.137589\u001b[0m\n",
      "\u001b[31m[06/21/2018 03:59:58 INFO 140419976746816] processed a total of 312 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4845.793008804321, \"sum\": 4845.793008804321, \"min\": 4845.793008804321}}, \"EndTime\": 1529553598.609122, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1529553593.750014}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/21/2018 03:59:58 INFO 140419976746816] #throughput_metric: host=algo-1, train throughput=64.3844286474 records/second\u001b[0m\n",
      "\u001b[31m[06/21/2018 03:59:58 INFO 140419976746816] #progress_metric: host=algo-1, completed 35 % of epochs\u001b[0m\n",
      "\u001b[31m[06/21/2018 03:59:58 INFO 140419976746816] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[06/21/2018 03:59:58 INFO 140419976746816] Saved checkpoint to \"/opt/ml/model/state_f6688be0-a3e3-4c73-8e25-dfed691af283-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 38.94186019897461, \"sum\": 38.94186019897461, \"min\": 38.94186019897461}}, \"EndTime\": 1529553598.648445, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1529553598.609189}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/21/2018 03:59:59 INFO 140419976746816] Epoch[7] Batch[0] avg_epoch_loss=4.611410\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:00:01 INFO 140419976746816] Epoch[7] Batch[5] avg_epoch_loss=4.287245\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:00:01 INFO 140419976746816] Epoch[7] Batch [5]#011Speed: 63.73 samples/sec#011loss=4.287245\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:00:03 INFO 140419976746816] Epoch[7] Batch[10] avg_epoch_loss=4.517816\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:00:03 INFO 140419976746816] Epoch[7] Batch [10]#011Speed: 81.37 samples/sec#011loss=4.794501\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:00:03 INFO 140419976746816] processed a total of 324 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 5017.136096954346, \"sum\": 5017.136096954346, \"min\": 5017.136096954346}}, \"EndTime\": 1529553603.678878, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1529553598.648495}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:00:03 INFO 140419976746816] #throughput_metric: host=algo-1, train throughput=64.5772999056 records/second\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:00:03 INFO 140419976746816] #progress_metric: host=algo-1, completed 40 % of epochs\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:00:03 INFO 140419976746816] loss did not improve for 1 epochs\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:00:04 INFO 140419976746816] Epoch[8] Batch[0] avg_epoch_loss=4.912075\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:00:06 INFO 140419976746816] Epoch[8] Batch[5] avg_epoch_loss=4.514730\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:00:06 INFO 140419976746816] Epoch[8] Batch [5]#011Speed: 66.64 samples/sec#011loss=4.514730\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:00:08 INFO 140419976746816] Epoch[8] Batch[10] avg_epoch_loss=4.457705\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:00:08 INFO 140419976746816] Epoch[8] Batch [10]#011Speed: 74.84 samples/sec#011loss=4.389276\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:00:08 INFO 140419976746816] processed a total of 326 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4997.39408493042, \"sum\": 4997.39408493042, \"min\": 4997.39408493042}}, \"EndTime\": 1529553608.690467, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1529553603.678948}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:00:08 INFO 140419976746816] #throughput_metric: host=algo-1, train throughput=65.2321595811 records/second\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:00:08 INFO 140419976746816] #progress_metric: host=algo-1, completed 45 % of epochs\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:00:08 INFO 140419976746816] loss did not improve for 2 epochs\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:00:09 INFO 140419976746816] Epoch[9] Batch[0] avg_epoch_loss=4.370693\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:00:11 INFO 140419976746816] Epoch[9] Batch[5] avg_epoch_loss=4.516114\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:00:11 INFO 140419976746816] Epoch[9] Batch [5]#011Speed: 69.00 samples/sec#011loss=4.516114\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:00:13 INFO 140419976746816] Epoch[9] Batch[10] avg_epoch_loss=4.349238\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:00:13 INFO 140419976746816] Epoch[9] Batch [10]#011Speed: 73.13 samples/sec#011loss=4.148987\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:00:13 INFO 140419976746816] processed a total of 324 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4946.341991424561, \"sum\": 4946.341991424561, \"min\": 4946.341991424561}}, \"EndTime\": 1529553613.651591, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1529553608.690571}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:00:13 INFO 140419976746816] #throughput_metric: host=algo-1, train throughput=65.5014266438 records/second\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:00:13 INFO 140419976746816] #progress_metric: host=algo-1, completed 50 % of epochs\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:00:13 INFO 140419976746816] loss did not improve for 3 epochs\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:00:14 INFO 140419976746816] Epoch[10] Batch[0] avg_epoch_loss=4.030439\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:00:16 INFO 140419976746816] Epoch[10] Batch[5] avg_epoch_loss=3.950729\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:00:16 INFO 140419976746816] Epoch[10] Batch [5]#011Speed: 70.47 samples/sec#011loss=3.950729\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:00:18 INFO 140419976746816] Epoch[10] Batch[10] avg_epoch_loss=4.002383\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:00:18 INFO 140419976746816] Epoch[10] Batch [10]#011Speed: 71.98 samples/sec#011loss=4.064367\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:00:18 INFO 140419976746816] processed a total of 334 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4922.390937805176, \"sum\": 4922.390937805176, \"min\": 4922.390937805176}}, \"EndTime\": 1529553618.589299, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1529553613.651667}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:00:18 INFO 140419976746816] #throughput_metric: host=algo-1, train throughput=67.8515779358 records/second\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:00:18 INFO 140419976746816] #progress_metric: host=algo-1, completed 55 % of epochs\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:00:18 INFO 140419976746816] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:00:18 INFO 140419976746816] Saved checkpoint to \"/opt/ml/model/state_904788c1-ae8b-4b13-9647-c8b787251d19-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 38.37084770202637, \"sum\": 38.37084770202637, \"min\": 38.37084770202637}}, \"EndTime\": 1529553618.628132, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1529553618.58938}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:00:19 INFO 140419976746816] Epoch[11] Batch[0] avg_epoch_loss=4.200814\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:00:21 INFO 140419976746816] Epoch[11] Batch[5] avg_epoch_loss=3.996786\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:00:21 INFO 140419976746816] Epoch[11] Batch [5]#011Speed: 68.11 samples/sec#011loss=3.996786\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:00:23 INFO 140419976746816] Epoch[11] Batch[10] avg_epoch_loss=4.079951\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:00:23 INFO 140419976746816] Epoch[11] Batch [10]#011Speed: 76.47 samples/sec#011loss=4.179748\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:00:23 INFO 140419976746816] processed a total of 327 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4902.810096740723, \"sum\": 4902.810096740723, \"min\": 4902.810096740723}}, \"EndTime\": 1529553623.544842, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1529553618.628179}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:00:23 INFO 140419976746816] #throughput_metric: host=algo-1, train throughput=66.694842001 records/second\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:00:23 INFO 140419976746816] #progress_metric: host=algo-1, completed 60 % of epochs\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:00:23 INFO 140419976746816] loss did not improve for 1 epochs\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:00:24 INFO 140419976746816] Epoch[12] Batch[0] avg_epoch_loss=3.900193\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:00:26 INFO 140419976746816] Epoch[12] Batch[5] avg_epoch_loss=3.875869\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:00:26 INFO 140419976746816] Epoch[12] Batch [5]#011Speed: 66.67 samples/sec#011loss=3.875869\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[06/21/2018 04:00:28 INFO 140419976746816] Epoch[12] Batch[10] avg_epoch_loss=3.990212\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:00:28 INFO 140419976746816] Epoch[12] Batch [10]#011Speed: 83.41 samples/sec#011loss=4.127423\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:00:28 INFO 140419976746816] processed a total of 338 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4972.790002822876, \"sum\": 4972.790002822876, \"min\": 4972.790002822876}}, \"EndTime\": 1529553628.531075, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1529553623.544921}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:00:28 INFO 140419976746816] #throughput_metric: host=algo-1, train throughput=67.9685267084 records/second\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:00:28 INFO 140419976746816] #progress_metric: host=algo-1, completed 65 % of epochs\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:00:28 INFO 140419976746816] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:00:28 INFO 140419976746816] Saved checkpoint to \"/opt/ml/model/state_65fa3b6a-a1b6-44c8-ad6f-6ee72af60ca8-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 39.025068283081055, \"sum\": 39.025068283081055, \"min\": 39.025068283081055}}, \"EndTime\": 1529553628.570557, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1529553628.531138}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:00:29 INFO 140419976746816] Epoch[13] Batch[0] avg_epoch_loss=4.054262\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:00:31 INFO 140419976746816] Epoch[13] Batch[5] avg_epoch_loss=3.835935\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:00:31 INFO 140419976746816] Epoch[13] Batch [5]#011Speed: 66.29 samples/sec#011loss=3.835935\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:00:33 INFO 140419976746816] Epoch[13] Batch[10] avg_epoch_loss=3.967947\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:00:33 INFO 140419976746816] Epoch[13] Batch [10]#011Speed: 80.30 samples/sec#011loss=4.126361\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:00:33 INFO 140419976746816] processed a total of 337 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4900.902032852173, \"sum\": 4900.902032852173, \"min\": 4900.902032852173}}, \"EndTime\": 1529553633.484756, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1529553628.570613}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:00:33 INFO 140419976746816] #throughput_metric: host=algo-1, train throughput=68.7612126717 records/second\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:00:33 INFO 140419976746816] #progress_metric: host=algo-1, completed 70 % of epochs\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:00:33 INFO 140419976746816] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:00:33 INFO 140419976746816] Saved checkpoint to \"/opt/ml/model/state_9d2df883-5639-4f6f-991f-ec234da68ce3-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 41.32699966430664, \"sum\": 41.32699966430664, \"min\": 41.32699966430664}}, \"EndTime\": 1529553633.526553, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1529553633.484833}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:00:34 INFO 140419976746816] Epoch[14] Batch[0] avg_epoch_loss=4.186512\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:00:36 INFO 140419976746816] Epoch[14] Batch[5] avg_epoch_loss=4.177978\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:00:36 INFO 140419976746816] Epoch[14] Batch [5]#011Speed: 68.27 samples/sec#011loss=4.177978\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:00:38 INFO 140419976746816] Epoch[14] Batch[10] avg_epoch_loss=4.325283\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:00:38 INFO 140419976746816] Epoch[14] Batch [10]#011Speed: 78.57 samples/sec#011loss=4.502049\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:00:38 INFO 140419976746816] processed a total of 322 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4919.841051101685, \"sum\": 4919.841051101685, \"min\": 4919.841051101685}}, \"EndTime\": 1529553638.460276, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1529553633.526616}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:00:38 INFO 140419976746816] #throughput_metric: host=algo-1, train throughput=65.44732789 records/second\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:00:38 INFO 140419976746816] #progress_metric: host=algo-1, completed 75 % of epochs\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:00:38 INFO 140419976746816] loss did not improve for 1 epochs\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:00:39 INFO 140419976746816] Epoch[15] Batch[0] avg_epoch_loss=4.157241\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:00:41 INFO 140419976746816] Epoch[15] Batch[5] avg_epoch_loss=4.088483\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:00:41 INFO 140419976746816] Epoch[15] Batch [5]#011Speed: 68.50 samples/sec#011loss=4.088483\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:00:43 INFO 140419976746816] Epoch[15] Batch[10] avg_epoch_loss=4.221059\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:00:43 INFO 140419976746816] Epoch[15] Batch [10]#011Speed: 78.64 samples/sec#011loss=4.380150\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:00:43 INFO 140419976746816] processed a total of 340 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 4941.30802154541, \"sum\": 4941.30802154541, \"min\": 4941.30802154541}}, \"EndTime\": 1529553643.415208, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1529553638.460348}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:00:43 INFO 140419976746816] #throughput_metric: host=algo-1, train throughput=68.8060884024 records/second\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:00:43 INFO 140419976746816] #progress_metric: host=algo-1, completed 80 % of epochs\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:00:43 INFO 140419976746816] loss did not improve for 2 epochs\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:00:43 INFO 140419976746816] Epoch[16] Batch[0] avg_epoch_loss=4.606621\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:00:46 INFO 140419976746816] Epoch[16] Batch[5] avg_epoch_loss=4.067633\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:00:46 INFO 140419976746816] Epoch[16] Batch [5]#011Speed: 71.62 samples/sec#011loss=4.067633\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:00:48 INFO 140419976746816] Epoch[16] Batch[10] avg_epoch_loss=4.097037\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:00:48 INFO 140419976746816] Epoch[16] Batch [10]#011Speed: 69.03 samples/sec#011loss=4.132321\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:00:48 INFO 140419976746816] processed a total of 322 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 5029.868125915527, \"sum\": 5029.868125915527, \"min\": 5029.868125915527}}, \"EndTime\": 1529553648.458792, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1529553643.415285}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:00:48 INFO 140419976746816] #throughput_metric: host=algo-1, train throughput=64.0161325108 records/second\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:00:48 INFO 140419976746816] #progress_metric: host=algo-1, completed 85 % of epochs\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:00:48 INFO 140419976746816] loss did not improve for 3 epochs\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:00:49 INFO 140419976746816] Epoch[17] Batch[0] avg_epoch_loss=3.933382\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:00:51 INFO 140419976746816] Epoch[17] Batch[5] avg_epoch_loss=3.770738\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:00:51 INFO 140419976746816] Epoch[17] Batch [5]#011Speed: 61.12 samples/sec#011loss=3.770738\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:00:53 INFO 140419976746816] Epoch[17] Batch[10] avg_epoch_loss=3.833585\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:00:53 INFO 140419976746816] Epoch[17] Batch [10]#011Speed: 73.17 samples/sec#011loss=3.909001\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:00:53 INFO 140419976746816] processed a total of 330 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 5359.143972396851, \"sum\": 5359.143972396851, \"min\": 5359.143972396851}}, \"EndTime\": 1529553653.835819, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1529553648.458867}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:00:53 INFO 140419976746816] #throughput_metric: host=algo-1, train throughput=61.5757684468 records/second\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:00:53 INFO 140419976746816] #progress_metric: host=algo-1, completed 90 % of epochs\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:00:53 INFO 140419976746816] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:00:53 INFO 140419976746816] Saved checkpoint to \"/opt/ml/model/state_893cf526-6aa9-4244-b5ee-48ec97c59543-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 42.756080627441406, \"sum\": 42.756080627441406, \"min\": 42.756080627441406}}, \"EndTime\": 1529553653.87903, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1529553653.835891}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:00:54 INFO 140419976746816] Epoch[18] Batch[0] avg_epoch_loss=3.872855\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:00:56 INFO 140419976746816] Epoch[18] Batch[5] avg_epoch_loss=3.708773\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:00:56 INFO 140419976746816] Epoch[18] Batch [5]#011Speed: 61.59 samples/sec#011loss=3.708773\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[06/21/2018 04:00:59 INFO 140419976746816] Epoch[18] Batch[10] avg_epoch_loss=3.907319\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:00:59 INFO 140419976746816] Epoch[18] Batch [10]#011Speed: 78.06 samples/sec#011loss=4.145574\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:00:59 INFO 140419976746816] processed a total of 328 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 5136.688947677612, \"sum\": 5136.688947677612, \"min\": 5136.688947677612}}, \"EndTime\": 1529553659.03071, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1529553653.879098}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:00:59 INFO 140419976746816] #throughput_metric: host=algo-1, train throughput=63.8529580615 records/second\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:00:59 INFO 140419976746816] #progress_metric: host=algo-1, completed 95 % of epochs\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:00:59 INFO 140419976746816] loss did not improve for 1 epochs\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:00:59 INFO 140419976746816] Epoch[19] Batch[0] avg_epoch_loss=3.532754\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:01:01 INFO 140419976746816] Epoch[19] Batch[5] avg_epoch_loss=3.858963\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:01:01 INFO 140419976746816] Epoch[19] Batch [5]#011Speed: 70.58 samples/sec#011loss=3.858963\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:01:04 INFO 140419976746816] Epoch[19] Batch[10] avg_epoch_loss=3.785638\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:01:04 INFO 140419976746816] Epoch[19] Batch [10]#011Speed: 68.24 samples/sec#011loss=3.697649\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:01:04 INFO 140419976746816] processed a total of 347 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 5062.340021133423, \"sum\": 5062.340021133423, \"min\": 5062.340021133423}}, \"EndTime\": 1529553664.106529, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1529553659.030785}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:01:04 INFO 140419976746816] #throughput_metric: host=algo-1, train throughput=68.5438974569 records/second\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:01:04 INFO 140419976746816] #progress_metric: host=algo-1, completed 100 % of epochs\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:01:04 INFO 140419976746816] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:01:04 INFO 140419976746816] Saved checkpoint to \"/opt/ml/model/state_f482ef2b-93f7-4c95-8524-e590e0dc2370-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 41.02206230163574, \"sum\": 41.02206230163574, \"min\": 41.02206230163574}}, \"EndTime\": 1529553664.147959, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1529553664.106603}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:01:04 INFO 140419976746816] Loading parameters from best epoch (19)\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.deserialize.time\": {\"count\": 1, \"max\": 14.400005340576172, \"sum\": 14.400005340576172, \"min\": 14.400005340576172}}, \"EndTime\": 1529553664.162551, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1529553664.14803}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:01:04 INFO 140419976746816] Final loss: 3.78563839739 (occured at epoch 19)\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:01:04 INFO 140419976746816] #quality_metric: host=algo-1, train final_loss <loss>=3.78563839739\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:01:04 INFO 140419976746816] Creating prediction graph.\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"get_graph.time\": {\"count\": 1, \"max\": 2506.143093109131, \"sum\": 2506.143093109131, \"min\": 2506.143093109131}}, \"EndTime\": 1529553666.669101, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1529553664.162596}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:01:08 INFO 140419976746816] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"finalize.time\": {\"count\": 1, \"max\": 4707.53288269043, \"sum\": 4707.53288269043, \"min\": 4707.53288269043}}, \"EndTime\": 1529553668.870463, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1529553666.669162}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:01:08 INFO 140419976746816] Serializing to /opt/ml/model/model_algo-1\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:01:09 INFO 140419976746816] Saved checkpoint to \"/opt/ml/model/model_algo-1-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"model.serialize.time\": {\"count\": 1, \"max\": 348.6020565032959, \"sum\": 348.6020565032959, \"min\": 348.6020565032959}}, \"EndTime\": 1529553669.219183, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1529553668.870538}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:01:09 INFO 140419976746816] Successfully created prediction graph and serialized the model.\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:01:09 INFO 140419976746816] Evaluating model accuracy on testset using 100 samples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"model.bind.time\": {\"count\": 1, \"max\": 0.034809112548828125, \"sum\": 0.034809112548828125, \"min\": 0.034809112548828125}}, \"EndTime\": 1529553669.220446, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1529553669.219236}\n",
      "\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"model.score.time\": {\"count\": 1, \"max\": 12364.542007446289, \"sum\": 12364.542007446289, \"min\": 12364.542007446289}}, \"EndTime\": 1529553681.584953, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1529553669.22049}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:01:21 INFO 140419976746816] #test_score (algo-1, wQuantileLoss[0.7]): 0.0255962\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:01:21 INFO 140419976746816] #test_score (algo-1, wQuantileLoss[0.6]): 0.0269393\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:01:21 INFO 140419976746816] #test_score (algo-1, mean_wQuantileLoss): 0.0262786\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:01:21 INFO 140419976746816] #test_score (algo-1, wQuantileLoss[0.1]): 0.0194958\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:01:21 INFO 140419976746816] #test_score (algo-1, RMSE): 7.21890483211\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:01:21 INFO 140419976746816] #test_score (algo-1, wQuantileLoss[0.3]): 0.0299864\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:01:21 INFO 140419976746816] #test_score (algo-1, wQuantileLoss[0.9]): 0.0218984\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:01:21 INFO 140419976746816] #test_score (algo-1, wQuantileLoss[0.2]): 0.0267579\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:01:21 INFO 140419976746816] #test_score (algo-1, wQuantileLoss[0.8]): 0.0259351\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:01:21 INFO 140419976746816] #test_score (algo-1, wQuantileLoss[0.5]): 0.0293708\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:01:21 INFO 140419976746816] #test_score (algo-1, wQuantileLoss[0.4]): 0.0305275\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:01:21 INFO 140419976746816] #quality_metric: host=algo-1, test mean_wQuantileLoss <loss>=0.0262785982341\u001b[0m\n",
      "\u001b[31m[06/21/2018 04:01:21 INFO 140419976746816] #quality_metric: host=algo-1, test RMSE <loss>=7.21890483211\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"totaltime\": {\"count\": 1, \"max\": 121637.42303848267, \"sum\": 121637.42303848267, \"min\": 121637.42303848267}, \"setuptime\": {\"count\": 1, \"max\": 8.651018142700195, \"sum\": 8.651018142700195, \"min\": 8.651018142700195}}, \"EndTime\": 1529553682.117442, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1529553681.585015}\n",
      "\u001b[0m\n",
      "===== Job Complete =====\n",
      "Billable seconds: 235\n"
     ]
    }
   ],
   "source": [
    "data_channels = {\n",
    "    \"train\": \"s3://{}/train/\".format(s3_data_path),\n",
    "    \"test\": \"s3://{}/test/\".format(s3_data_path)\n",
    "}\n",
    "\n",
    "estimator.fit(inputs=data_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create endpoint and predictor\n",
    "Now that we have trained a model, we can use it to perform predictions by deploying it to an endpoint.\n",
    "\n",
    "Note: remember to delete the enpoint after running this experiment. A cell at the very bottom of this notebook will do that: make sure you run it at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating endpoint with name test-demo-deepar-2018-06-21-03-55-48-846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---"
     ]
    }
   ],
   "source": [
    "job_name = estimator.latest_training_job.name\n",
    "\n",
    "endpoint_name = sagemaker_session.endpoint_from_job(\n",
    "    job_name=job_name,\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m4.xlarge',\n",
    "    deployment_image=image_name,\n",
    "    role=role\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepARPredictor(sagemaker.predictor.RealTimePredictor):\n",
    "\n",
    "    def set_prediction_parameters(self, freq, prediction_length):\n",
    "        \"\"\"Set the time frequency and prediction length parameters. This method **must** be called\n",
    "        before being able to use `predict`.\n",
    "        \n",
    "        Parameters:\n",
    "        freq -- string indicating the time frequency\n",
    "        prediction_length -- integer, number of predicted time points\n",
    "        \n",
    "        Return value: none.\n",
    "        \"\"\"\n",
    "        self.freq = freq\n",
    "        self.prediction_length = prediction_length\n",
    "        \n",
    "    def predict(self, ts, cat=None, encoding=\"utf-8\", num_samples=100, quantiles=[\"0.1\", \"0.5\", \"0.9\"]):\n",
    "        \"\"\"Requests the prediction of for the time series listed in `ts`, each with the (optional)\n",
    "        corresponding category listed in `cat`.\n",
    "        \n",
    "        Parameters:\n",
    "        ts -- list of `pandas.Series` objects, the time series to predict\n",
    "        cat -- list of integers (default: None)\n",
    "        encoding -- string, encoding to use for the request (default: \"utf-8\")\n",
    "        num_samples -- integer, number of samples to compute at prediction time (default: 100)\n",
    "        quantiles -- list of strings specifying the quantiles to compute (default: [\"0.1\", \"0.5\", \"0.9\"])\n",
    "        \n",
    "        Return value: list of `pandas.DataFrame` objects, each containing the predictions\n",
    "        \"\"\"\n",
    "        prediction_times = [x.index[-1]+1 for x in ts]\n",
    "        req = self.__encode_request(ts, cat, encoding, num_samples, quantiles)\n",
    "        res = super(DeepARPredictor, self).predict(req)\n",
    "        return self.__decode_response(res, prediction_times, encoding)\n",
    "    \n",
    "    def __encode_request(self, ts, cat, encoding, num_samples, quantiles):\n",
    "        instances = [series_to_obj(ts[k], cat[k] if cat else None) for k in range(len(ts))]\n",
    "        configuration = {\"num_samples\": num_samples, \"output_types\": [\"quantiles\"], \"quantiles\": quantiles}\n",
    "        http_request_data = {\"instances\": instances, \"configuration\": configuration}\n",
    "        return json.dumps(http_request_data).encode(encoding)\n",
    "    \n",
    "    def __decode_response(self, response, prediction_times, encoding):\n",
    "        response_data = json.loads(response.decode(encoding))\n",
    "        list_of_df = []\n",
    "        for k in range(len(prediction_times)):\n",
    "            prediction_index = pd.DatetimeIndex(start=prediction_times[k], freq=self.freq, periods=self.prediction_length)\n",
    "            list_of_df.append(pd.DataFrame(data=response_data['predictions'][k]['quantiles'], index=prediction_index))\n",
    "        return list_of_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = DeepARPredictor(\n",
    "    endpoint=endpoint_name,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    content_type=\"application/json\"\n",
    ")\n",
    "predictor.set_prediction_parameters(freq, prediction_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_time_series_training = []\n",
    "for ts in time_series_training:\n",
    "    new_time_series_training.append(ts.asfreq('T'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_time_series_test = []\n",
    "for ts in time_series_test:\n",
    "    new_time_series_test.append(ts.asfreq('T'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 max for memory (on my desktop)\n",
    "\n",
    "list_of_df  = predictor.predict(new_time_series_training[:2]) # predicted forecast\n",
    "actual_data = new_time_series_test # full data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for k in range(len(list_of_df)): \n",
    "    plt.figure(figsize=(12,6))\n",
    "    actual_data[k][-prediction_length-context_length:].plot(label='target')\n",
    "    p10 = list_of_df[k]['0.1'] \n",
    "    p90 = list_of_df[k]['0.9'] #set limits predictively\n",
    "    plt.fill_between(p10.index, p10, p90, color='y', alpha=0.5, label='80% confidence interval')\n",
    "    list_of_df[k]['0.5'].plot(label='prediction median') # set requests for capacity allocation \n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "two paths:\n",
    "** 1. iterating and improving model fit\n",
    "    a. try to get it to full data set- as big as possible \n",
    "    b. get test metric !!! - if large enough use RMSE\n",
    "    c. use model to make predictions (hold out data or ?)\n",
    "    \n",
    "2. look at p90- upper bound in graph = max capacity set in real time, cost calc between this and actual capacity that was provisioned, \n",
    "\n",
    "3. extra: try to get in hyperparameter tuning \n",
    "\n",
    "4. put final cleaned up notebook\n",
    "\n",
    "5. accessing data and documentation MAKE CLEAR notes on "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Created hyperparameter tuning job on the GUI, but attempting to do with code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_input_train = 's3://{}/{}/train'.format(bucket, prefix)\n",
    "s3_input_validation ='s3://{}/{}/validation/'.format(bucket, prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 706,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'number' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-706-596473968352>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m       },\n\u001b[1;32m     29\u001b[0m       \"ResourceLimits\": { \n\u001b[0;32m---> 30\u001b[0;31m          \u001b[0;34m\"MaxNumberOfTrainingJobs\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnumber\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m          \u001b[0;34m\"MaxParallelTrainingJobs\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnumber\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m       },\n",
      "\u001b[0;31mNameError\u001b[0m: name 'number' is not defined"
     ]
    }
   ],
   "source": [
    "tuning_job_config = {\n",
    "   \"HyperParameterTuningJobConfig\": { \n",
    "      \"HyperParameterTuningJobObjective\": { \n",
    "         \"MetricName\": \"string\",\n",
    "         \"Type\": \"string\"\n",
    "      },\n",
    "      \"ParameterRanges\": { \n",
    "         \"CategoricalParameterRanges\": [ \n",
    "            { \n",
    "               \"Name\": \"string\",\n",
    "               \"Values\": [ \"string\" ]\n",
    "            }\n",
    "         ],\n",
    "         \"ContinuousParameterRanges\": [ \n",
    "            { \n",
    "               \"MaxValue\": \"string\",\n",
    "               \"MinValue\": \"string\",\n",
    "               \"Name\": \"string\"\n",
    "            }\n",
    "         ],\n",
    "         \"IntegerParameterRanges\": [ \n",
    "            { \n",
    "               \"MaxValue\": \"string\",\n",
    "               \"MinValue\": \"string\",\n",
    "               \"Name\": \"string\"\n",
    "            }\n",
    "         ]\n",
    "      },\n",
    "      \"ResourceLimits\": { \n",
    "         \"MaxNumberOfTrainingJobs\": number,\n",
    "         \"MaxParallelTrainingJobs\": number\n",
    "      },\n",
    "      \"Strategy\": \"string\"\n",
    "   },\n",
    "   \"HyperParameterTuningJobName\": \"string\",\n",
    "   \"Tags\": [ \n",
    "      { \n",
    "         \"Key\": \"string\",\n",
    "         \"Value\": \"string\"\n",
    "      }\n",
    "   ],\n",
    "   \"TrainingJobDefinition\": { \n",
    "      \"AlgorithmSpecification\": { \n",
    "         \"MetricDefinitions\": [ \n",
    "            { \n",
    "               \"Name\": \"string\",\n",
    "               \"Regex\": \"string\"\n",
    "            }\n",
    "         ],\n",
    "         \"TrainingImage\": \"string\",\n",
    "         \"TrainingInputMode\": \"string\"\n",
    "      },\n",
    "      \"InputDataConfig\": [ \n",
    "         { \n",
    "            \"ChannelName\": \"string\",\n",
    "            \"CompressionType\": \"string\",\n",
    "            \"ContentType\": \"string\",\n",
    "            \"DataSource\": { \n",
    "               \"S3DataSource\": { \n",
    "                  \"S3DataDistributionType\": \"string\",\n",
    "                  \"S3DataType\": \"string\",\n",
    "                  \"S3Uri\": \"string\"\n",
    "               }\n",
    "            },\n",
    "            \"RecordWrapperType\": \"string\"\n",
    "         }\n",
    "      ],\n",
    "      \"OutputDataConfig\": { \n",
    "         \"KmsKeyId\": \"string\",\n",
    "         \"S3OutputPath\": \"string\"\n",
    "      },\n",
    "      \"ResourceConfig\": { \n",
    "         \"InstanceCount\": number,\n",
    "         \"InstanceType\": \"string\",\n",
    "         \"VolumeKmsKeyId\": \"string\",\n",
    "         \"VolumeSizeInGB\": number\n",
    "      },\n",
    "      \"RoleArn\": \"string\",\n",
    "      \"StaticHyperParameters\": { \n",
    "         \"string\" : \"string\" \n",
    "      },\n",
    "      \"StoppingCondition\": { \n",
    "         \"MaxRuntimeInSeconds\": number\n",
    "      },\n",
    "      \"VpcConfig\": { \n",
    "         \"SecurityGroupIds\": [ \"string\" ],\n",
    "         \"Subnets\": [ \"string\" ]\n",
    "      }\n",
    "   }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "metadata": {},
   "outputs": [],
   "source": [
    "smclient = boto3.Session().client('sagemaker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 705,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tuning_job_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-705-85ec1923f517>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m smclient.create_hyper_parameter_tuning_job(HyperParameterTuningJobName = tuning_job_name,\n\u001b[0m\u001b[1;32m      2\u001b[0m                                             \u001b[0mHyperParameterTuningJobConfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuning_job_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                             TrainingJobDefinition = training_job_definition)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tuning_job_name' is not defined"
     ]
    }
   ],
   "source": [
    "smclient.create_hyper_parameter_tuning_job(HyperParameterTuningJobName = tuning_job_name,\n",
    "                                            HyperParameterTuningJobConfig = tuning_job_config,\n",
    "                                            TrainingJobDefinition = training_job_definition)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query a trained model by using the model's endpoint. The endpoint takes the following JSON request format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "{\n",
    " \"instances\": [\n",
    "  { \"start\": \"2009-11-01 00:00:00\", \"target\": [4.0, 10.0, 50.0, 100.0, 113.0], \"cat\": 0},\n",
    "  { \"start\": \"2012-01-30\", \"target\": [1.0], \"cat\": 2 },\n",
    "  { \"start\": \"1999-01-30\", \"target\": [2.0, 1.0], \"cat\": 1 }\n",
    " ],\n",
    " \"configuration\": {\n",
    "  \"num_samples\": 50,\n",
    "  \"output_types\": [\"mean\", \"quantiles\", \"samples\"],\n",
    "  \"quantiles\": [\"0.5\", \"0.9\"]\n",
    " }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'endpoint_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-ff2ef6ceb0b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendpoint_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'endpoint_name' is not defined"
     ]
    }
   ],
   "source": [
    "sagemaker_session.delete_endpoint(endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
